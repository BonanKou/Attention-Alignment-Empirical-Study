{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import krippendorff\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import csv \n",
    "import copy\n",
    "\n",
    "def calculate_precision(true_labels, predicted_labels):\n",
    "    return precision_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "def calculate_recall(true_labels, predicted_labels):\n",
    "    return recall_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "def calculate_f1(true_labels, predicted_labels):\n",
    "    return f1_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "def calculate_cohens_kappa(rater1, rater2):\n",
    "    return cohen_kappa_score(rater1, rater2)\n",
    "\n",
    "def find_substring_bounds(s, sub):\n",
    "    start = s.find(sub)\n",
    "    if start != -1:\n",
    "        end = start + len(sub)\n",
    "        return (start, end)\n",
    "    else:\n",
    "        return (-1, -1)\n",
    "\n",
    "def get_model_dict(attention_dict, threshold, ground_truth_dict):\n",
    "    model_dict = {}\n",
    "    for i in attention_dict:\n",
    "        temp = copy.deepcopy(ground_truth_dict[i])\n",
    "        temp = sorted(temp, key=lambda item: item[1], reverse=True)\n",
    "        top_keywords = [j[2] for j in temp[:threshold]]\n",
    "        model_dict[i] = top_keywords\n",
    "    return model_dict\n",
    "\n",
    "def compare_index_ranges(ground_truth_dict, comparison_dict):\n",
    "    overlap_results = {}\n",
    "    for prompt, range_dict in ground_truth_dict.items():\n",
    "        result = [0] * len(range_dict)\n",
    "        if prompt in comparison_dict:\n",
    "            comp_ranges = comparison_dict[prompt]\n",
    "            gt_ranges = [j[2] for j in range_dict]\n",
    "            for i, gt_range in enumerate(gt_ranges):\n",
    "                for comp_range in comp_ranges:\n",
    "                    if (gt_range[1] > comp_range[0] and gt_range[0] < comp_range[1]):\n",
    "                        result[i] = 1\n",
    "                        break\n",
    "            overlap_results[prompt] = result\n",
    "    return overlap_results\n",
    "\n",
    "def concat_dict_values(dict_to_concat):\n",
    "    concatenated_list = []\n",
    "    for value_list in dict_to_concat.values():\n",
    "        concatenated_list.extend(value_list)\n",
    "    return concatenated_list\n",
    "\n",
    "def tokenize_with_indices(attention_dict):\n",
    "    result = dict()\n",
    "    for i in attention_dict:\n",
    "        temp = []\n",
    "        current = 0\n",
    "        for j in attention_dict[i]:\n",
    "            start = i[current:].find(j[0])\n",
    "            if start != -1:\n",
    "                end = start + len(j[0])\n",
    "                temp.append((j[0], j[1], (start + current, end + current)))\n",
    "                current += end\n",
    "        result[i] = temp\n",
    "    return result\n",
    "\n",
    "def tokenize_with_indices_incoder(attention_dict):\n",
    "    result = dict()\n",
    "    for i in attention_dict:\n",
    "        character_to_character_map = {}\n",
    "        \n",
    "        cursor = 0\n",
    "\n",
    "        for char_index, char in enumerate(i):\n",
    "            character_to_character_map[cursor] = char_index\n",
    "            if i != \" \":\n",
    "                cursor += 1\n",
    "        \n",
    "        temp = []\n",
    "        current = 0\n",
    "        for j in attention_dict[i]:\n",
    "            start = i.replace(\" \", \"\")[current:].find(j[0])\n",
    "            if start != -1:\n",
    "                end = start + len(j[0])\n",
    "                temp.append((j[0], j[1], (character_to_character_map[start + current], character_to_character_map[end + current])))\n",
    "                current += end\n",
    "        result[i] = temp\n",
    "    return result\n",
    "\n",
    "def save_to_csv(filename, data):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def overlap_length(range1, range2):\n",
    "    # Unpack ranges\n",
    "    a, b = range1\n",
    "    c, d = range2\n",
    "\n",
    "    # Calculate the start and end of the overlap\n",
    "    start_overlap = max(a, c)\n",
    "    end_overlap = min(b, d)\n",
    "\n",
    "    # Calculate the overlap length\n",
    "    if start_overlap < end_overlap:\n",
    "        return end_overlap - start_overlap\n",
    "    else:\n",
    "        # No overlap\n",
    "        return 0\n",
    "    \n",
    "def calculate_new_overall(attention_dict, threshold, incoder=False):\n",
    "    human_dict = pickle.load(open(\"gradient_based/dataset.pkl\", \"rb\"))\n",
    "\n",
    "    # print(\"Human dict\", human_dict)\n",
    "\n",
    "    ground_truth_dict = {}\n",
    "\n",
    "    if not incoder:\n",
    "        ground_truth_dict = tokenize_with_indices(attention_dict)\n",
    "    else:\n",
    "        ground_truth_dict = tokenize_with_indices_incoder(attention_dict)\n",
    "\n",
    "    for real_key in attention_dict:\n",
    "        attention_dict[real_key] = sorted(attention_dict[real_key], key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    model_dict = get_model_dict(attention_dict, threshold, ground_truth_dict)\n",
    "\n",
    "    groundtruth_human_overlap = compare_index_ranges(ground_truth_dict, human_dict)\n",
    "\n",
    "    groundtruth_model_overlap = compare_index_ranges(ground_truth_dict, model_dict)\n",
    "\n",
    "    concat_human = concat_dict_values(groundtruth_human_overlap)\n",
    "\n",
    "    concat_model = concat_dict_values(groundtruth_model_overlap)\n",
    "    \n",
    "    def ka(a, b):\n",
    "        return krippendorff.alpha(reliability_data=[a, b], level_of_measurement='nominal')\n",
    "\n",
    "    krippendorf_alpha = krippendorff.alpha(reliability_data=[concat_human, concat_model], level_of_measurement='nominal')\n",
    "\n",
    "    def calculate_sm_precision(h, m):\n",
    "        h = set(h)\n",
    "        m = set(m)\n",
    "        summation = 0\n",
    "        for human_keyword in h:\n",
    "            for model_keyword in m:\n",
    "                if model_keyword[1] - model_keyword[0] != 0:\n",
    "                    summation += overlap_length(human_keyword, model_keyword) / (model_keyword[1] - model_keyword[0])\n",
    "        return summation\n",
    "    \n",
    "    def calculate_sm_recall(h, m):\n",
    "        h = set(h)\n",
    "        m = set(m)\n",
    "        summation = 0\n",
    "        for human_keyword in h:\n",
    "            for model_keyword in m:\n",
    "                if human_keyword[1] - human_keyword[0] == 0:\n",
    "                    summation += 0\n",
    "                else:\n",
    "                    summation += overlap_length(human_keyword, model_keyword) / (human_keyword[1] - human_keyword[0])\n",
    "        return summation\n",
    "\n",
    "    def calculate_sm_f1(precision, recall):\n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    def ka(a, b):\n",
    "        return krippendorff.alpha(reliability_data=[a, b], level_of_measurement='nominal')\n",
    "\n",
    "    all_human = []\n",
    "    all_model = []\n",
    "    so_far = 0\n",
    "    for index, i in enumerate(model_dict):\n",
    "        h = human_dict[i]\n",
    "        m = model_dict[i]\n",
    "        all_human += [(each_human[0] + so_far, each_human[1] + so_far) for each_human in h]\n",
    "        all_model += [(each_model[0] + so_far, each_model[1] + so_far) for each_model in m]\n",
    "        so_far += len(i)\n",
    "\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    for index, i in enumerate(model_dict):\n",
    "        h = human_dict[i]\n",
    "        m = model_dict[i]\n",
    "\n",
    "        precision_sum += calculate_sm_precision(h, m)\n",
    "        recall_sum += calculate_sm_recall(h, m)\n",
    "\n",
    "    precision = precision_sum / len(all_model)\n",
    "    recall = recall_sum / len(all_human)\n",
    "    f1 = calculate_sm_f1(precision, recall)\n",
    "    statistics = [\n",
    "        f\"{precision:.1%}\",   # 将精确度格式化为百分比，并在%前添加反斜杠\n",
    "        f\"{recall:.1%}\",      # 将召回率格式化为百分比，并在%前添加反斜杠\n",
    "        f\"{f1:.1%}\",          # 将F1分数格式化为百分比，并在%前添加反斜杠\n",
    "        f\"{krippendorf_alpha:.2f}\"  # 将Krippendorff's alpha系数格式化为保留两位小数的浮点数\n",
    "    ]\n",
    "    return statistics, [precision, recall, f1, krippendorf_alpha]\n",
    "\n",
    "def calculate_final(model, calculate, method):\n",
    "    if method.find(\"self_attention\") !=-1:\n",
    "        attention_dict = pickle.load(open(f\"/home/bonan/fse2024/{method}code/{method}{model}.pkl\", \"rb\"))\n",
    "        result = []\n",
    "        for key in attention_dict:\n",
    "            for real_key in attention_dict[key]:\n",
    "                attention_dict[key][real_key] = sorted(attention_dict[key][real_key], key=lambda item: item[1], reverse=True)\n",
    "            five = calculate(attention_dict[key], 5)\n",
    "            ten = calculate(attention_dict[key], 10)\n",
    "            twenty = calculate(attention_dict[key], 20)\n",
    "            result += five + ten + twenty\n",
    "    elif method.find(\"bert\") != -1:\n",
    "        attention_dict = pickle.load(open(f\"/home/bonan/fse2024/{method}code/{method}{model}.pkl\", \"rb\"))\n",
    "        result = []\n",
    "        for real_key in attention_dict:\n",
    "            attention_dict[real_key] = sorted(attention_dict[real_key], key=lambda item: item[1], reverse=True)\n",
    "        five = calculate(attention_dict, 5)\n",
    "        ten = calculate(attention_dict, 10)\n",
    "        twenty = calculate(attention_dict, 20)\n",
    "        result += five + ten + twenty\n",
    "    elif method.find(\"shap\") != -1:\n",
    "        attention_dict = pickle.load(open(f\"/home/bonan/fse2024/shap/{method}{model}.pkl\", \"rb\"))\n",
    "        result = []\n",
    "        for real_key in attention_dict:\n",
    "            attention_dict[real_key] = sorted(attention_dict[real_key], key=lambda item: item[1], reverse=True)\n",
    "        five = calculate(attention_dict, 5)\n",
    "        ten = calculate(attention_dict, 10)\n",
    "        twenty = calculate(attention_dict, 20)\n",
    "        result += five + ten + twenty\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2 A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/bonan/fse2024/human_eval/dataset.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m methods:\n\u001b[0;32m     12\u001b[0m     attention \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 13\u001b[0m     five, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_new_overall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     ten, _ \u001b[38;5;241m=\u001b[39m calculate_new_overall(copy\u001b[38;5;241m.\u001b[39mdeepcopy(attention), \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     15\u001b[0m     twenty, _ \u001b[38;5;241m=\u001b[39m calculate_new_overall(copy\u001b[38;5;241m.\u001b[39mdeepcopy(attention), \u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 126\u001b[0m, in \u001b[0;36mcalculate_new_overall\u001b[1;34m(attention_dict, threshold, incoder)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_new_overall\u001b[39m(attention_dict, threshold, incoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 126\u001b[0m     human_dict \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/bonan/fse2024/human_eval/dataset.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# print(\"Human dict\", human_dict)\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     ground_truth_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\16382\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/bonan/fse2024/human_eval/dataset.pkl'"
     ]
    }
   ],
   "source": [
    "models = ['incoder', 'codegen', 'codeparrot', 'gptj', 'polycoder', 'gpt4']\n",
    "# models = ['incoder']\n",
    "methods = [\"bert_perturbation\", \"shap_perturbation\"]\n",
    "prefix = \"experiment_result\"\n",
    "files = []\n",
    "\n",
    "table_content = []\n",
    "\n",
    "for model in models:\n",
    "    each_model_result = []\n",
    "    for method in methods:\n",
    "        attention = pickle.load(open(f\"{prefix}/{method}_{model}.pkl\", \"rb\"))\n",
    "        five, _ = calculate_new_overall(copy.deepcopy(attention), 5)\n",
    "        ten, _ = calculate_new_overall(copy.deepcopy(attention), 10)\n",
    "        twenty, _ = calculate_new_overall(copy.deepcopy(attention), 20)\n",
    "        each_model_result += five + ten + twenty\n",
    "    table_content.append(each_model_result)\n",
    "    print(model, each_model_result)\n",
    "\n",
    "save_to_csv(f\"perturbation.csv\", table_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2 B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['incoder', 'codegen', 'codeparrot', 'gptj', 'polycoder']\n",
    "methods = [\"inputxgradient_reading\", \"inputxgradient_coding\"]\n",
    "\n",
    "prefix = \"../experiment_results\"\n",
    "\n",
    "files = []\n",
    "table_content = []\n",
    "\n",
    "for model in models:\n",
    "    each_model_result = []\n",
    "    for method in methods:\n",
    "        attention = pickle.load(open(f\"{prefix}/{method}_{model}.pkl\", \"rb\"))\n",
    "        five, _ = calculate_new_overall(copy.deepcopy(attention), 5, model==\"incoder\")\n",
    "        ten, _ = calculate_new_overall(copy.deepcopy(attention), 10, model==\"incoder\")\n",
    "        twenty, _ = calculate_new_overall(copy.deepcopy(attention), 20, model==\"incoder\")\n",
    "        each_model_result += five + ten + twenty\n",
    "    table_content.append(each_model_result)\n",
    "\n",
    "methods = [\"saliency_reading\", \"saliency_coding\"]\n",
    "for model in models:\n",
    "    each_model_result = []\n",
    "    for method in methods:\n",
    "        attention = pickle.load(open(f\"{prefix}/{method}_{model}.pkl\", \"rb\"))\n",
    "        five, _ = calculate_new_overall(copy.deepcopy(attention), 5)\n",
    "        ten, _ = calculate_new_overall(copy.deepcopy(attention), 10)\n",
    "        twenty, _ = calculate_new_overall(copy.deepcopy(attention), 20)\n",
    "        each_model_result += five + ten + twenty\n",
    "    table_content.append(each_model_result)\n",
    "save_to_csv(f\"last_day/gradient.csv\", table_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2 C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['incoder', 'codegen', 'codeparrot', 'gptj', 'polycoder']\n",
    "methods = [\"reading_first\", \"coding_first\"]\n",
    "\n",
    "prefix = \"../experiment_results\"\n",
    "\n",
    "files = []\n",
    "table_content = []\n",
    "\n",
    "for model in models:\n",
    "    each_model_result = []\n",
    "    for method in methods:\n",
    "        attention = pickle.load(open(f\"{prefix}/{method}_{model}.pkl\", \"rb\"))\n",
    "        five, _ = calculate_new_overall(copy.deepcopy(attention), 5)\n",
    "        ten, _ = calculate_new_overall(copy.deepcopy(attention), 10)\n",
    "        twenty, _ = calculate_new_overall(copy.deepcopy(attention), 20)\n",
    "        each_model_result += five + ten + twenty\n",
    "    table_content.append(each_model_result)\n",
    "\n",
    "methods = [\"reading_last\", \"coding_last\"]\n",
    "for model in models:\n",
    "    each_model_result = []\n",
    "    for method in methods:\n",
    "        attention = pickle.load(open(f\"{prefix}/{method}_{model}.pkl\", \"rb\"))\n",
    "        five, _ = calculate_new_overall(copy.deepcopy(attention), 5)\n",
    "        ten, _ = calculate_new_overall(copy.deepcopy(attention), 10)\n",
    "        twenty, _ = calculate_new_overall(copy.deepcopy(attention), 20)\n",
    "        each_model_result += five + ten + twenty\n",
    "    table_content.append(each_model_result)\n",
    "\n",
    "methods = [\"reading_all\", \"coding_all\"]\n",
    "for model in models:\n",
    "    each_model_result = []\n",
    "    for method in methods:\n",
    "        attention = pickle.load(open(f\"{prefix}/{method}_{model}.pkl\", \"rb\"))\n",
    "        five, _ = calculate_new_overall(copy.deepcopy(attention), 5)\n",
    "        ten, _ = calculate_new_overall(copy.deepcopy(attention), 10)\n",
    "        twenty, _ = calculate_new_overall(copy.deepcopy(attention), 20)\n",
    "        each_model_result += five + ten + twenty\n",
    "    table_content.append(each_model_result)\n",
    "\n",
    "save_to_csv(f\"self-attention.csv\", table_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['incoder', 'codegen', 'codeparrot', 'gptj', 'polycoder']\n",
    "methods = [\"bert_perturbation\", \n",
    "           \"shap_perturbation\",\n",
    "           \"inputxgradient_reading\", \n",
    "           \"inputxgradient_coding\",\n",
    "           \"saliency_reading\", \n",
    "           \"saliency_coding\",\n",
    "           \"reading_first\", \n",
    "           \"reading_last\", \n",
    "           \"reading_all\", \n",
    "           \"coding_first\", \n",
    "           \"coding_last\", \n",
    "           \"coding_all\"]\n",
    "prefix = \"../experiment_results\"\n",
    "\n",
    "gradient_based = [\n",
    "                \"inputxgradient_reading\", \n",
    "                \"inputxgradient_coding\",\n",
    "                \"saliency_reading\", \n",
    "                \"saliency_coding\"\n",
    "                ]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "all_methods_models = []\n",
    "for method in methods:\n",
    "    each_method = []\n",
    "    for model in models:\n",
    "        incoder = False\n",
    "        if method in gradient_based and model == \"incoder\":\n",
    "            incoder = True\n",
    "        attention = pickle.load(open(f\"{prefix}/{method}_{model}.pkl\", \"rb\"))\n",
    "        each_model_stats = []\n",
    "        _, five = calculate_new_overall(copy.deepcopy(attention), 5, incoder)\n",
    "        _, ten = calculate_new_overall(copy.deepcopy(attention), 10, incoder)\n",
    "        _, twenty = calculate_new_overall(copy.deepcopy(attention), 20, incoder)\n",
    "        each_model_stats += five[2:] + ten[2:] + twenty[2:]\n",
    "        each_method.append(each_model_stats)\n",
    "    each_method_raw_average = np.mean(np.array(each_method), axis=0)\n",
    "    each_method_string = [\n",
    "        f\"{each_method_raw_average[0]:.1%}\",\n",
    "        f\"{each_method_raw_average[1]:.2f}\",\n",
    "        f\"{each_method_raw_average[2]:.1%}\",\n",
    "        f\"{each_method_raw_average[3]:.2f}\",\n",
    "        f\"{each_method_raw_average[4]:.1%}\",\n",
    "        f\"{each_method_raw_average[5]:.2f}\" \n",
    "    ]\n",
    "\n",
    "    all_methods_models.append(each_method_string)\n",
    "save_to_csv(f\"comparison.csv\", all_methods_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2 Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['incoder', 'codegen', 'codeparrot', 'gptj', 'polycoder']\n",
    "methods = [\"bert_perturbation\", \n",
    "           \"shap_perturbation\",\n",
    "           \"inputxgradient_reading\", \n",
    "           \"inputxgradient_coding\",\n",
    "           \"saliency_reading\", \n",
    "           \"saliency_coding\",\n",
    "           \"reading_first\", \n",
    "           \"reading_last\", \n",
    "           \"reading_all\", \n",
    "           \"coding_first\", \n",
    "           \"coding_last\", \n",
    "           \"coding_all\"]\n",
    "prefix = \"../experiment_results\"\n",
    "\n",
    "gradient_based = [\n",
    "                \"inputxgradient_reading\", \n",
    "                \"inputxgradient_coding\",\n",
    "                \"saliency_reading\", \n",
    "                \"saliency_coding\"\n",
    "                ]\n",
    "\n",
    "import numpy as np\n",
    "all_methods_models = []\n",
    "for model in models:\n",
    "    for k in [5, 10, 20]:\n",
    "        precision = []\n",
    "        recall = []\n",
    "        f1 = []\n",
    "        ka = []\n",
    "        for method in methods:\n",
    "            attention = pickle.load(open(f\"{prefix}/{method}_{model}.pkl\", \"rb\"))\n",
    "            if method in gradient_based and model==\"incoder\":\n",
    "                _, metrics = calculate_new_overall(copy.deepcopy(attention), k, True)\n",
    "            else:\n",
    "                _, metrics = calculate_new_overall(copy.deepcopy(attention), k)\n",
    "            precision.append(metrics[0])\n",
    "            recall.append(metrics[1])\n",
    "            f1.append(metrics[2])\n",
    "            ka.append(metrics[3])\n",
    "        \n",
    "        precision_max_index = precision.index(max(precision))\n",
    "        recall_max_index = recall.index(max(recall))\n",
    "        f1_max_index = f1.index(max(f1))\n",
    "        ka_max_index = ka.index(max(ka))\n",
    "\n",
    "        print(f\"For model {model} at k={k}, max precision: {methods[precision_max_index]}, number is {max(precision)}\")\n",
    "        print(f\"For model {model} at k={k}, max recall: {methods[recall_max_index]}, number is {max(recall)}\")\n",
    "        print(f\"For model {model} at k={k}, max f1: {methods[f1_max_index]}, number is {max(f1)}\")\n",
    "        print(f\"For model {model} at k={k}, max ka: {methods[ka_max_index]}, number is {max(ka)}\")\n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fse2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
