{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import shap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model=\"gpt4\"):\n",
    "    if model == \"gpt4\":\n",
    "        client = openai.OpenAI(\n",
    "            api_key= \"\",\n",
    "        )\n",
    "       \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            model=\"gpt-4-1106-preview\"\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(r\"dataset.pkl\", \"rb\"))\n",
    "original_results = pickle.load(open(r\"original_results.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "    \n",
    "def generate_perturbed_texts(tokens, prefix=\"\", postfix=\"\"):\n",
    "    perturbed_texts = []\n",
    "    for idx in range(len(tokens)):\n",
    "        perturbed_tokens = tokens.copy()\n",
    "        perturbed_tokens[idx] = \"[MASK]\"\n",
    "        perturbed_text = \" \".join(perturbed_tokens)\n",
    "        perturbed_texts.append(prefix + perturbed_text + postfix)\n",
    "    return perturbed_texts\n",
    "\n",
    "def compare(original_code, perturbated_code):\n",
    "    return 1 - sentence_bleu([original_code.split()], perturbated_code.split())\n",
    "\n",
    "def each_shap(original_text, original_output):\n",
    "    def model_function(binary_vectors, perturbed_texts, original_output):\n",
    "        explanations = []\n",
    "        for binary_vector in binary_vectors:\n",
    "            perturbed_text = perturbed_texts[np.argmax(np.all(binary_matrix == binary_vector, axis=1))]\n",
    "            perturbed_output = llm_with_caching(perturbed_text)\n",
    "            score = compare(original_output, perturbed_output)\n",
    "            explanations.append(score)\n",
    "        return np.array(explanations)\n",
    "\n",
    "    cache = {}\n",
    "    def llm_with_caching(perturbed_text):\n",
    "        if perturbed_text in cache:\n",
    "            return cache[perturbed_text]\n",
    "        else:\n",
    "            cache[perturbed_text] = llm(perturbed_text)\n",
    "        return cache[perturbed_text]\n",
    "    \n",
    "    if original_text.find(\">>>\") != -1:\n",
    "        tokens = tokenizer(original_text[original_text.find(\"def\"):original_text.find(\">>>\")])\n",
    "    else:\n",
    "        tokens = tokenizer(original_text)\n",
    "    \n",
    "    if original_text.find(\">>>\") != -1:\n",
    "        background_dataset = generate_perturbed_texts(tokens, original_text[:original_text.find(\"def\")], original_text[original_text.find(\">>>\"):])\n",
    "    else:\n",
    "        background_dataset = generate_perturbed_texts(tokens)\n",
    "\n",
    "    binary_matrix = []\n",
    "    for perturbed_text in background_dataset:\n",
    "        row = [1 if token not in perturbed_text.split() or token == \"[MASK]\" else 0 for token in tokens]\n",
    "        binary_matrix.append(row)\n",
    "    binary_matrix = np.array(binary_matrix)\n",
    "\n",
    "    explainer = shap.KernelExplainer(lambda x: model_function(x, background_dataset, original_output), binary_matrix)\n",
    "    shap_values = explainer.shap_values(np.ones((1, len(tokens))), nsamples=50)  # Explaining with all tokens \"present\"\n",
    "\n",
    "    importances = []\n",
    "\n",
    "    for token, shap_value in zip(tokens, shap_values[0]):\n",
    "        importances.append((token, shap_value))\n",
    "        \n",
    "    return original_text, importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_item(item):\n",
    "    if item not in human_eval_shap_results:\n",
    "        original_text = item\n",
    "        original_output = original_results[item]\n",
    "        return each_shap(original_text, original_output)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "human_eval_shap_results = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:\n",
    "    futures = [executor.submit(process_item, item) for item in dataset]\n",
    "\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        if future.result():\n",
    "            human_eval_shap_results[future.result()[0]] = future.result()[1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
